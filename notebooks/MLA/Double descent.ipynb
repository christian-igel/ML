{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Double Descent \n",
    "## Christian Igel, 2024\n",
    "\n",
    "This code is based on the [Double Descent Demystified: Identifying, Interpreting & Ablating the Sources of a Deep Learning Puzzle](https://arxiv.org/abs/2303.14151) by\n",
    "Rylan Schaeffer, Mikail Khona, Zachary Robertson, Akhilan Boopathy, Kateryna Pistunova, Jason W. Rocks, Ila Rani Fiete, Oluwasanmi Koyejo.  I am happy for suggestions to improve the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to fit\n",
    "Just some artificial function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_gen(N_samples, resolution=500):\n",
    "    def func(x):\n",
    "        return 2*x+np.sin(25*x)\n",
    "    x_min = -1.\n",
    "    x_max = 1.\n",
    "    xline = np.linspace(x_min, x_max, resolution)\n",
    "    x = np.random.choice(xline, size=N_samples)\n",
    "    y = func(x)\n",
    "    yline = func(xline)\n",
    "    return (x.reshape(-1,1), y, xline.reshape(-1,1), yline)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training data\n",
    "np.random.seed(1)  # play with the seed\n",
    "N = 30\n",
    "X_train_raw, y_train, X_test_raw, y_test = func_gen(N)\n",
    "\n",
    "# Plot function and sampled points\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(X_test_raw, y_test,c='b',lw=2, label='f')\n",
    "plt.scatter(X_train_raw, y_train, edgecolors='k', c='orange', s=50, label='train', zorder=10)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Legendre polynomial\n",
    "\n",
    "We use a polynomial feature embedding. The type of polynomial features matters. \n",
    "We consider Legendre polynomials, which look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_degree = 4\n",
    "xlin = np.linspace(-1, 1, 101).reshape(-1, 1)\n",
    "leg = np.polynomial.legendre.legval(xlin, np.eye(max_degree + 1), tensor=False)\n",
    "for _d in range(0, max_degree + 1):\n",
    "    plt.plot(xlin, leg[:,_d], label=r'$P_{}(x)$'.format(_d))\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression using polynomial embedding\n",
    "\n",
    "This function perfromns linear regression using polynomial embedding.\n",
    "One can specify the `degree` of the polynomial and thetype, where `std` refers to the standard polynomial embedding and `Legendre` to using Legendre polynomials.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_it(degree = 10, poly_type='std', verbose=False):\n",
    "    if poly_type=='Legendre':\n",
    "        X_train = np.polynomial.legendre.legval(X_train_raw, np.eye(degree+1), tensor=False)\n",
    "        X_test = np.polynomial.legendre.legval(X_test_raw, np.eye(degree+1), tensor=False)\n",
    "    else:\n",
    "        poly = PolynomialFeatures(degree=degree, interaction_only=False, include_bias=True)\n",
    "        X_train = poly.fit_transform(X_train_raw)\n",
    "        X_test = poly.transform(X_test_raw)\n",
    "\n",
    "    N, d = X_train.shape\n",
    "        \n",
    "    # Solve for the best possible approximate solution in terms of least squares using SVD\n",
    "    U, S, Vt = np.linalg.svd(X_train, full_matrices=False)\n",
    "    beta = Vt.T @ np.linalg.inv(np.diag(S)) @ U.T @ y_train\n",
    "    # Alternative: beta = np.linalg.pinv(X_train, rcond=1e-16) @ y_train\n",
    "        \n",
    "    norm = np.linalg.norm(beta)\n",
    "    \n",
    "    # perform train and test inference\n",
    "    y_pred = X_train @ beta\n",
    "    y_pred_test = X_test @ beta\n",
    "    \n",
    "    return (y_pred, y_pred_test, norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_type='Legendre'\n",
    "#poly_type='std'\n",
    "e_train = []\n",
    "e_test = []\n",
    "beta_norm = []\n",
    "\n",
    "for degree in range(0, 101):\n",
    "    y_pred, y_pred_test, norm = do_it(degree, poly_type=poly_type)\n",
    "    if(degree == N-1):\n",
    "        y_pred_unique, y_pred_test_unique= y_pred, y_pred_test\n",
    "    e_train.append(mean_squared_error(y_pred, y_train))\n",
    "    e_test.append(mean_squared_error(y_pred_test, y_test))\n",
    "    beta_norm.append(norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot results. The plot is cut at 0.001 to hide numerical trouble (uncomment `plt.ylim(0.001)` to face the truth).\n",
    "To compute the norm, we view all points as living in the space with the highest dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(e_train, label = 'train', lw=2);\n",
    "plt.plot(e_test, label = 'test', lw=2);\n",
    "plt.plot(beta_norm, label = 'weight norm', lw=2);\n",
    "plt.yscale('log')\n",
    "plt.axvline(x = N-1, color = 'k', linestyle='dashed', label = 'degree=$N$-1')\n",
    "plt.grid(True)\n",
    "plt.xlabel('degree (number of parameters - 1)')\n",
    "plt.ylabel('MSE / weight norm')\n",
    "plt.ylim(0.001)\n",
    "plt.legend()\n",
    "plt.xlim(-2,102)\n",
    "#plt.savefig(\"doubleDescent.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization of the solution with the most features and the one with as many features (degree plus 1) as training data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "plt.ylim(-3, 3)\n",
    "plt.plot(X_test_raw, y_test, color='C1', label='true function', lw=2)\n",
    "plt.plot(X_test_raw, y_pred_test, c='r',label=\"final prediction\", lw=2)\n",
    "plt.plot(X_test_raw, y_pred_test_unique, c='g',label=\"degree=$N$-1\", lw=2)\n",
    "plt.scatter(X_train_raw, y_train, c='C0', s=50, label=\"training data\", zorder=5)\n",
    "plt.xlim(-1,1)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "#plt.savefig(\"doubleDescentFit.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
