{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "### Christian Igel, 2024\n",
    "\n",
    "\n",
    "I took inspiration from https://github.com/tirthajyoti/Interactive_Machine_Learning by Tirthajyoti Sarkar.\n",
    "I am happy for suggestions to improve the notebook. You may want to set `plt.rcParams['text.usetex']` to `False` if you run into LaTeX problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, interactive, IntSlider, Layout, interact_manual, fixed\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['text.usetex'] = True  # set to False if you run into LaTeX problems\n",
    "plt.rc('text.latex', preamble=r'\\usepackage{amsmath,amsfonts,bm}')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LassoCV, Lasso\n",
    "from sklearn.linear_model import RidgeCV, Ridge\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression and Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to fit\n",
    "Just some artificial function with Gaussian noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_gen(N_samples, noise_sd, resolution=500):\n",
    "    \"\"\"Data generator.\n",
    "    Parameters:\n",
    "        N_samples: data set size\n",
    "        noise_sd: std. deviation of additive zero mean Gaussin noise\n",
    "        resolution: number of equally spaced points for plotting noise-free function\n",
    "    Returns: \n",
    "        x, y: inputs and (noisy) targets\n",
    "        x_lin, y_lin: equally spaced inputs and their noise-free function values \n",
    "        x_min, x_max: input domain\n",
    "    \"\"\"    \n",
    "    def func(x):\n",
    "        return 2*x -0.6*x**2 + 0.2*x**3+12 * np.sin(x)\n",
    "    x_min = -5\n",
    "    x_max = 5\n",
    "    x_lin = np.linspace(x_min, x_max, resolution)  # equally spaced points\n",
    "    x = np.random.choice(x_lin, size=N_samples)  # random points\n",
    "    y = func(x)\n",
    "    y_lin = func(x_lin)\n",
    "    y = y + np.random.normal(scale=noise_sd, size=N_samples)\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(x_lin,y_lin,c='C1',lw=2)\n",
    "    plt.scatter(x,y,c='C0',s=60)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    return (x.reshape(-1,1), y, x_lin, y_lin, x_min, x_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call the interactive widget with the data generating function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = interactive(func_gen, \n",
    "                N_samples=widgets.IntSlider(min=10, max=200., step=10, continuous_update=False, value=100, description='#samples'),\n",
    "                noise_sd=widgets.FloatSlider(min=0., max=10., step=0.5, continuous_update=False, value=2.5, description='noise sd'), resolution=fixed(500))\n",
    "display(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the data from the plot\n",
    "x, y, x_lin, y_lin, x_min, x_max = p.result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear models encapsulated in a function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_eps = 0.01\n",
    "lasso_nalpha = 20  # alpha = lambda (= gamma)\n",
    "lasso_iter = 100000  # maximum number of iterative LASSO optimization steps   \n",
    "lasso_tol = 0.0001  # controls a stopping criterion of the LASSO optimization\n",
    "\n",
    "def func_fit(model_type, test_size, degree, alpha, resolution=500):\n",
    "    \"\"\"Fits model to data.\n",
    "    Parameters:\n",
    "        model_type: 'Linear regression', 'Ridge', or 'LASSO'\n",
    "        test_size: percentage of test data\n",
    "        degree: degree of polynomial\n",
    "        alpha: natural logarithm of regularization parameter\n",
    "        resolution: number of equally spaced points for plotting \n",
    "    Returns: \n",
    "        train_score, test_score: training and test score (R2)\n",
    "        model, model_type: model and given model type\n",
    "    \"\"\"    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=test_size, random_state=55)\n",
    "    \n",
    "    if (model_type=='Linear regression'):\n",
    "        model = make_pipeline(PolynomialFeatures(degree,interaction_only=False), \n",
    "                              LinearRegression(fit_intercept=False))\n",
    "    if (model_type=='Ridge'):    \n",
    "        model = make_pipeline(PolynomialFeatures(degree,interaction_only=False), \n",
    "                              Ridge(alpha=np.exp(alpha), fit_intercept=False, solver='svd'))\n",
    "    if (model_type=='LASSO'):    \n",
    "        model = make_pipeline(PolynomialFeatures(degree,interaction_only=False), \n",
    "                              Lasso(alpha=np.exp(alpha), fit_intercept=False, max_iter=lasso_iter, warm_start=True, positive=False, selection='random'))\n",
    "        \n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit_transform(X_train)\n",
    "    scaler.transform(X_test)\n",
    "    \n",
    "    model.fit(X_train,y_train)\n",
    "    \n",
    "    train_pred = np.array(model.predict(X_train))\n",
    "    train_score = model.score(X_train,y_train)\n",
    "    \n",
    "    test_pred = np.array(model.predict(X_test))\n",
    "    test_score = model.score(X_test, y_test)\n",
    "    \n",
    "    X_grid = np.linspace(x_min, x_max, resolution).reshape(-1,1)\n",
    "    y_grid = np.array(model.predict(X_grid))\n",
    "    \n",
    "    plt.figure(figsize=(14,6))\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.title(r\"Test $R^2$ score: %.3f\"%(test_score))\n",
    "    plt.xlabel(r\"$x$\", fontsize=14)\n",
    "    plt.ylabel(r\"$y$\", fontsize=14)\n",
    "    plt.scatter(X_test, y_test, c='C0', s=60, label='Actual test values')\n",
    "    plt.scatter(X_test, test_pred,c='C1', s=40, label='Predicted values')\n",
    "    plt.plot(X_grid, y_grid,label=\"Model\", c='C1', lw=2)\n",
    "    y_min = np.min([y_test.min(), test_pred.min()])\n",
    "    y_max = np.min([y_test.max(), test_pred.max()])\n",
    "    plt.ylim([np.amin([1.1*y_min, 0.9*y_min]), np.amax([1.1*y_max, 0.9*y_max])])\n",
    "    plt.xlim(np.min(X_grid), np.max(X_grid))\n",
    "    plt.plot(x_lin, y_lin, label=\"Function w/o noise\", c='red', lw=2, alpha=0.5)\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1,2,1)\n",
    "    plt.title(r\"Training $R^2$ score: %.3f\"%(train_score))\n",
    "    plt.xlabel(r\"$x$\", fontsize=14)\n",
    "    plt.ylabel(r\"$y$\", fontsize=14)\n",
    "    plt.scatter(X_train, y_train, c='C0', label=\"Traning data\", s=60)\n",
    "    plt.scatter(X_train, train_pred, c='C1', label=\"Fitted values\", s=40)\n",
    "    plt.plot(X_grid, y_grid, label=\"Model\", c='C1', lw=2)\n",
    "    y_min = np.min([y_train.min(), train_pred.min()])\n",
    "    y_max = np.min([y_train.max(), train_pred.max()])\n",
    "    plt.ylim([np.amin([1.1*y_min, 0.9*y_min]), np.amax([1.1*y_max, 0.9*y_max])])\n",
    "    plt.xlim(np.min(X_grid), np.max(X_grid))\n",
    "    plt.plot(x_lin, y_lin, label=\"Function w/o noise\", c='red', lw=2, alpha=0.5)\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    #plt.savefig(\"interactiveLinearRegression\"+str(degree)+\".pdf\",bbox_inches='tight')\n",
    "    plt.show()\n",
    "       \n",
    "    return (train_score, test_score, model, model_type)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the encapsulated ML function \n",
    "The score is the coefficient of determination $R^2$, the closer to 1 the better (see assignment).\n",
    "The regularization parameter is referred to as $\\lambda$ (the parameter of the Python function is called `alpha` and in the literature $\\gamma$ is often used). For some setting, the optimization algorithms do not converge (simply ignore the warning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style = {'description_width': 'initial'}\n",
    "# Continuous_update = False for IntSlider control to stop continuous model evaluation while the slider is being dragged\n",
    "m = interactive(func_fit,model_type=widgets.RadioButtons(options=['Linear regression','LASSO', 'Ridge'],\n",
    "                                                         description = \"Choose Model\",style=style,\n",
    "                                                         layout=Layout(width='250px')),\n",
    "                test_size=widgets.Dropdown(options={\"10% of data\":0.1,\"25% of data\":0.25, \"50% of data\":0.5, \"75% of data\":0.75},\n",
    "                                           description=\"Test set size\",style=style, value=0.5),\n",
    "                degree=widgets.IntSlider(min=1, max=40, step=1, description=r'Polynomial degree',\n",
    "                                       style=style, continuous_update=False),\n",
    "                alpha=widgets.FloatSlider(min=-15, max=10, step=1, description=r'ln λ',\n",
    "                                       style=style, continuous_update=False),\n",
    "                resolution=fixed(500))\n",
    "\n",
    "# Display the control\n",
    "display(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "e_train, e_text, model, model_type = m.result\n",
    "print(model[1].coef_)\n",
    "\n",
    "plt.xlabel('Parameter number (0 is intercept)')\n",
    "plt.ylabel('Parameter value')\n",
    "plt.grid(axis='y')\n",
    "plt.xticks(np.arange(0, len(model[1].coef_)))\n",
    "plt.bar(np.arange(0, len(model[1].coef_)), model[1].coef_);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation\n",
    "Set parameters and parameter ranges, which should depend on whether the model type is LASSO or Ridge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_type=='Linear regression':  # our std. linear regression has no hyperparameters\n",
    "    model_type = 'Ridge'\n",
    "test_size = .30  # percentage of data used as (external) test set\n",
    "log_lambda_range = np.arange(-5,3)  # range of regularization parameters on logarithmic scale\n",
    "degree_range = np.arange(1,6)  # range of degrees for polynomial representation\n",
    "folds = 5  # number of folds for cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the cross-validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_degree = 0\n",
    "best_log_lambda = 0\n",
    "best_cv_score = 0\n",
    "\n",
    "# (X_train, y_train) are used for cross-validation\n",
    "# (X_test, y_test) are used for testing the model and is not used in training and model selection (cross-validation)\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=test_size,random_state=55)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "kf = KFold(n_splits=folds)\n",
    "\n",
    "for degree in degree_range:  # loop over degrees\n",
    "    # Compute polynomial features including constant (bias) feature (include_bias=True)\n",
    "    poly = PolynomialFeatures(degree=degree, interaction_only=False)\n",
    "    X_train_poly = poly.fit_transform(X_train)\n",
    "    for log_lambda in log_lambda_range:  # loop of regularization parameters\n",
    "        cv_score = 0\n",
    "        for train_i, val_i in kf.split(X_train_poly):  # loop over cross-validation folds\n",
    "            X_train_fold, y_train_fold = X_train_poly[train_i], y_train[train_i]\n",
    "            X_val_fold, y_val_fold = X_train_poly[val_i], y_train[val_i]\n",
    "    \n",
    "            # Only the current training data is used for scaling\n",
    "            scaler.fit_transform(X_train_fold)\n",
    "            scaler.transform(X_val_fold)\n",
    "     \n",
    "            # Features include constant (bias), so we need not fit an extra intercept \n",
    "            if (model_type=='Ridge'):\n",
    "                model = Ridge(alpha=np.exp(log_lambda), fit_intercept=False)\n",
    "            elif (model_type=='LASSO'):\n",
    "                model = Lasso(alpha=np.exp(log_lambda), fit_intercept=False, max_iter=lasso_iter)\n",
    "            else:\n",
    "                print(\"unknown model: only LASSO and Ridge are supported\")\n",
    "                assert(False)\n",
    "            model.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "            score = model.score(X_val_fold, y_val_fold)\n",
    "            cv_score += score\n",
    "        if cv_score > best_cv_score:  # found new best score? \n",
    "            best_cv_score = cv_score\n",
    "            best_degree = degree\n",
    "            best_log_lambda = log_lambda\n",
    "\n",
    "print(\"Best average CV score:\", best_cv_score / folds)\n",
    "print(model_type, \"model, degree:\", best_degree, \"log. lambda:\", best_log_lambda)\n",
    "\n",
    "# After cross-validation, the model \n",
    "#   is trained with the best parameters on the full training data set and \n",
    "#   is evaluated on the external test data\n",
    "poly = PolynomialFeatures(degree=best_degree, interaction_only=False)\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_test_poly  = poly.transform(X_test)\n",
    "scaler.fit_transform(X_train_poly)\n",
    "scaler.transform(X_test_poly)\n",
    "if (model_type=='Ridge'):\n",
    "    model = Ridge(alpha=np.exp(best_log_lambda), fit_intercept=False, solver='svd')\n",
    "elif (model_type=='LASSO'):\n",
    "    model = Lasso(alpha=np.exp(best_log_lambda), fit_intercept=False, max_iter=lasso_iter)\n",
    "else:\n",
    "    print(\"unknown model\")\n",
    "    assert(False)\n",
    "model.fit(X_train_poly, y_train)\n",
    "score = model.score(X_test_poly, y_test)\n",
    "\n",
    "print(\"Performance best model on test set:\", score)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
