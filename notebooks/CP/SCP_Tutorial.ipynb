{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c04e0e06106c3b4e",
   "metadata": {},
   "source": [
    "# Split conformal prediction for regression tutorial\n",
    "\n",
    "Mikolaj Mazurczyk & Christian Igel, 2025.\n",
    "Please let us know how to improve the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e26650864edc8",
   "metadata": {},
   "source": [
    "#### A standard way to estimate the performance of a predictive model:\n",
    "* To estimate the expected accuracy of a model, we estimate its accuracy on an i.i.d. validation data set not used during training.\n",
    "* In expectation over all draws of the validation data set, the mean accuracy on the validation data set equals the expected performance.\n",
    "* To account for finite sample effects, we apply finite sample concentration bounds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a758f908f1a5d66",
   "metadata": {},
   "source": [
    "#### What about the following for estimating the uncertainty of a model:\n",
    "* To estimate the uncertainty performance of a model, we estimate its uncertainty on an i.i.d. calibration data set not used during training.\n",
    "* In expectation over all draws of the calibration data set, the $\\alpha$-quantile of uncertainties on the calibration data should be the expected $\\alpha$-quantile of uncertainty.\n",
    "* To account for finite sample effects, we apply finite sample concentration bounds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aba23cc08b34c67",
   "metadata": {},
   "source": [
    "### Importing libraries & setting up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T16:49:31.942389Z",
     "start_time": "2025-08-05T16:49:27.002694Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from abc import ABC, abstractmethod\n",
    "from scipy.spatial.distance import pdist\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f048241d9d4bf4e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T16:49:31.954200Z",
     "start_time": "2025-08-05T16:49:31.952285Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set up fonts for plots\n",
    "plt.rcParams['text.usetex'] = True\n",
    "plt.rcParams['text.latex.preamble'] = '\\\\usepackage{amsmath}\\\\usepackage{amssymb}'\n",
    "params = {'text.usetex' : True,\n",
    "          'font.size' : 16,\n",
    "          'font.family' : 'lmodern'\n",
    "          }\n",
    "plt.rcParams.update(params)\n",
    "plt.rcParams[\"savefig.dpi\"] = 300\n",
    "# Reproducibility\n",
    "seed = 71\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed);\n",
    "\n",
    "# Make directory for figures\n",
    "os.makedirs(\"figures\", exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcaa83daf284084a",
   "metadata": {},
   "source": [
    "### Example problem\n",
    "\n",
    "The underlying model is a simple power clipped function $f(x)=\\min(0.1, ax^b)$ plus potentially heteroscedastic noise $\\epsilon_x$ drawn from $\\sigma_xN(0,1)$ with $\\sigma_x=\\sigma + x\\cdot\\sigma_{\\text{inc}}$. The Python function returns the noise-free (true) function values $t(x)=f(x)$ and the noisy observations $y(x)=f(x)+\\epsilon_x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6422f0931827b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T16:49:31.961472Z",
     "start_time": "2025-08-05T16:49:31.958995Z"
    }
   },
   "outputs": [],
   "source": [
    "def f(x, sigma = 0, sigma_inc = 0):\n",
    "    n = len(x)  # number of samples\n",
    "    a = 0.10  # scaling factor\n",
    "    b = 2  # exponent\n",
    "    t = a + np.power(x,b)  # true target values\n",
    "    s = x * sigma_inc + sigma  # standard deviations potentially depending on x\n",
    "    e = s * np.random.normal(0, 1, n)\n",
    "    y = np.clip(t+e, a_min=0.01, a_max=None)\n",
    "    return t, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2d649ce7282acb",
   "metadata": {},
   "source": [
    "Generate a uniformly distributed calibration data set $\\{(x_1,t_1,y_1),\\dots, (x_n,t_n,y_n)\\}$ and an evenly spaced test data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e4a1d8cf7805ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T16:49:31.973941Z",
     "start_time": "2025-08-05T16:49:31.966133Z"
    }
   },
   "outputs": [],
   "source": [
    "n_train_example = 100 # number of training data points\n",
    "n_test_example = 500 # number of testing data points\n",
    "n_cal_example = 100  # number of calibration data points\n",
    "sigma_example = 10  # standard deviation of noise\n",
    "sigma_inc_example = 1.5  # noise increase\n",
    "\n",
    "# Training set\n",
    "x_train_example = np.sort(np.random.uniform(0, 10, n_train_example))\n",
    "_, y_train_example = f(x_train_example, sigma_example, sigma_inc_example)\n",
    "# Calibration set\n",
    "x_cal_example = np.sort(np.random.uniform(0, 10, n_cal_example))\n",
    "_, y_cal_example = f(x_cal_example, sigma_example, sigma_inc_example)\n",
    "# Evenly spaced test data\n",
    "x_test_example = np.linspace(0, 10, n_test_example)\n",
    "t_test_example, y_test_example = f(x_test_example, sigma_example, sigma_inc_example)\n",
    "\n",
    "x_train_example, y_train_example = torch.tensor(x_train_example), torch.tensor(y_train_example)\n",
    "x_cal_example, y_cal_example = torch.tensor(x_cal_example), torch.tensor(y_cal_example)\n",
    "x_test_example, y_test_example = torch.tensor(x_test_example), torch.tensor(y_test_example)\n",
    "\n",
    "alpha_example = 0.1  # significance level for conformal prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cece9e02d475cd9",
   "metadata": {},
   "source": [
    "### Defining the model and a function for fitting it\n",
    "The model is a power regression model $\\hat{f}(x)=ax^b$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d0f146-fd69-4cca-953c-e7a7373ed85d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T16:49:32.369199Z",
     "start_time": "2025-08-05T16:49:32.365099Z"
    }
   },
   "outputs": [],
   "source": [
    "class PowerRegressionModel(torch.nn.Module):\n",
    "    def __init__(self, factor:float =1., exponent:float =1.):\n",
    "        super(PowerRegressionModel, self).__init__()\n",
    "        self.a = torch.nn.Parameter(torch.tensor(factor))\n",
    "        self.b = torch.nn.Parameter(torch.tensor(exponent))\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.a * torch.pow(x, self.b)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19842133-3697-47d6-ad8b-72fbc2853c66",
   "metadata": {},
   "source": [
    "We fit the model in log space (which is not uncommon) to get a pronounced asymmetric error distribution.\n",
    "That is, the pointwise loss function is $\\ell(y,\\hat{y}) = (\\ln y -\\ln \\hat{y})^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3553bd83-b93f-4712-8f2f-59c4d0158458",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T16:49:32.369199Z",
     "start_time": "2025-08-05T16:49:32.365099Z"
    }
   },
   "outputs": [],
   "source": [
    "def fit_power(x, y, a=1., b=1., lr=0.01, num_epochs = 2000, verbose=False):\n",
    "    model = PowerRegressionModel(factor=a, exponent=b)\n",
    "    model.train()\n",
    "    mse = nn.MSELoss()\n",
    "    optimizer = torch.optim.Rprop(model.parameters(), lr=lr, etas=(0.5, 1.1), step_sizes=(0, 5))\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        y_pred = model(x)\n",
    "        loss= mse(torch.log(y_pred), torch.log(y))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        if(verbose):\n",
    "            print('epoch {}, RMSE {}'.format(epoch, torch.sqrt(loss).item()))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405d428c5acb2e86",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T16:49:38.030666Z",
     "start_time": "2025-08-05T16:49:32.477197Z"
    }
   },
   "outputs": [],
   "source": [
    "model_example = fit_power(x_train_example, y_train_example, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f40b5b6352dc2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T16:49:38.045219Z",
     "start_time": "2025-08-05T16:49:38.041585Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Model parameters: (\", model_example.a.item(), \",\", model_example.b.item(), \")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efbb70e-282e-4c1e-bede-b7cd1f39c36c",
   "metadata": {},
   "source": [
    "Evaluate the model on training, calibration, and test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bde43527aec999",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T16:49:38.072697Z",
     "start_time": "2025-08-05T16:49:38.070143Z"
    }
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    y_model_train_example = model_example(x_train_example)\n",
    "    y_model_cal_example = model_example(x_cal_example)\n",
    "    y_model_test_example = model_example(x_test_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c687b86167e5eb2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T16:49:39.455518Z",
     "start_time": "2025-08-05T16:49:38.088378Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_model_predictions(x_test, y_model_test, x_train, y_train, x_cal=None, y_cal=None, filename=None):\n",
    "    plt.plot(x_test, y_model_test, label=\"model $\\\\hat{f}(x)$\", color=\"brown\")\n",
    "    plt.plot(x_train, y_train, '.', label=\"training data\", alpha=0.5, color=\"brown\")\n",
    "    plt.grid()\n",
    "    plt.xlabel(\"$x$\")\n",
    "    plt.ylabel(\"$y$\")\n",
    "    plt.margins(x=0)\n",
    "    plt.legend(loc=2)\n",
    "    if(filename):\n",
    "        plt.savefig(filename, bbox_inches='tight')\n",
    "\n",
    "plot_model_predictions(x_test_example, y_model_test_example, x_train_example, y_train_example, filename=\"figures/allo_model.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc4d72c81b5d6fc",
   "metadata": {},
   "source": [
    "### Basic idea of (split) conformal prediction (SCP)\n",
    "This is based on the tutorial by Anastasios N. Angelopoulos and Stephen Bates (2023), “Conformal Prediction: A Gentle Introduction”, *Foundations and Trends® in Machine Learning* 16(4):494–591, DOI: [10.1561/2200000101](http://dx.doi.org/10.1561/2200000101)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8cfe33beb0d108",
   "metadata": {},
   "source": [
    "#### Basic conformal prediction scheme:\n",
    "\n",
    "1. Identify a heuristic notion $u(x)$ of uncertainty using the pre-trained model\n",
    "2. Define the score function $s(x, y)$, where larger scores encode worse agreement between $x$ and $y$\n",
    "3. Compute $\\hat{q}$ as the $\\frac{\\lceil (n+1)(1−\\alpha)\\rceil}{n}$\n",
    "quantile of the calibration scores $\\{s_i,\\dots, s_n\\}$ with $s_i = s(x_i, y_i)$\n",
    "4. Use this quantile to form the prediction sets for new examples:\n",
    "$$\n",
    "\\mathcal{C}(x)=\\{ y:s(x,y) \\le \\hat{q} \\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7d4740eb27a098",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T16:49:39.482289Z",
     "start_time": "2025-08-05T16:49:39.477359Z"
    }
   },
   "outputs": [],
   "source": [
    "class BaseSCP(ABC):\n",
    "    def __init__(self, alpha):\n",
    "        self.alpha = alpha\n",
    "        self.emp_q = None\n",
    "        self.x_cal = None\n",
    "        self.y_cal = None\n",
    "        self.cal_scores = None\n",
    "\n",
    "    @abstractmethod\n",
    "    def score(self, x, y):\n",
    "        pass\n",
    "\n",
    "    def fit(self, x_cal, y_cal):\n",
    "        n = len(x_cal)\n",
    "        k = int(np.ceil((n + 1) * (1 - self.alpha)))\n",
    "        scores = self.score(x_cal, y_cal)\n",
    "        scores, _ = torch.sort(scores)\n",
    "        self.x_cal = x_cal\n",
    "        self.y_cal = y_cal\n",
    "        self.cal_scores = scores\n",
    "        if k > n:\n",
    "            self.emp_q = torch.inf\n",
    "        else:\n",
    "            self.emp_q = scores[k - 1]  # -1 for zero-based indexing\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, x_test):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d321fba9d278184",
   "metadata": {},
   "source": [
    "### Absolute error SCP\n",
    "\n",
    "First, assume we have no information about the uncertainty ($u(x)=\\text{const}$) and we simply define\n",
    "$s(x,y)=|y-f(x)|$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4c2cd24c14a41",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T16:49:39.497497Z",
     "start_time": "2025-08-05T16:49:39.495149Z"
    }
   },
   "outputs": [],
   "source": [
    "class AbsoluteErrorSCP(BaseSCP):\n",
    "    def __init__(self, model, alpha):\n",
    "        self.model = model\n",
    "        super().__init__(alpha)\n",
    "\n",
    "    def score(self, x, y):\n",
    "        with torch.no_grad():\n",
    "            return torch.abs(y - self.model(x))\n",
    "\n",
    "    def predict(self, x_test):\n",
    "        with torch.no_grad():\n",
    "            y_test_pred = self.model(x_test)\n",
    "        return y_test_pred - self.emp_q, y_test_pred + self.emp_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3511591ed0990a61",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T16:49:39.525407Z",
     "start_time": "2025-08-05T16:49:39.520224Z"
    }
   },
   "outputs": [],
   "source": [
    "abs_err_scp = AbsoluteErrorSCP(model_example, alpha_example)\n",
    "abs_err_scp.fit(x_cal_example, y_cal_example)\n",
    "abs_err_scp_prediction_sets = abs_err_scp.predict(x_test_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ee41fd-2190-48e0-86d5-15c79700566a",
   "metadata": {},
   "source": [
    "Visualize the absolute error scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934b63be-aea0-470e-bfb1-f20ea97a57e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_absolute_error_scores(x_test, y_model_test, x_cal, y_cal, y_model_cal, filename=None):\n",
    "    plt.plot(x_test, y_model_test, label=\"model $\\\\hat{f}(x)$\", color=\"brown\")\n",
    "    plt.plot(x_cal, y_model_cal, '.', color=\"brown\")\n",
    "    plt.plot(x_cal, y_cal, '.', label=r\"calibration data $\\mathcal{D}_{\\text{cal}}$\", alpha=0.5, color=\"green\")\n",
    "\n",
    "    plt.plot(np.vstack([x_cal, x_cal]), np.vstack([y_cal, y_model_cal]), '-', color='lightgreen', zorder=0)\n",
    "    plt.plot([],[], color=\"tab:green\", label='scores $s(x,y)=|y-\\\\hat{f}(x)|$')\n",
    "    plt.grid();\n",
    "    plt.xlabel(\"$x$\");\n",
    "    plt.ylabel(\"$y$\");\n",
    "    plt.margins(x=0)\n",
    "    plt.legend(loc=2);\n",
    "    if filename:\n",
    "        plt.savefig(filename, bbox_inches='tight');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafba233-d609-458c-8484-69e3f1ce6a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_absolute_error_scores(\n",
    "    x_test_example,\n",
    "    y_model_test_example,\n",
    "    x_cal_example, \n",
    "    y_cal_example,\n",
    "    y_model_cal_example,\n",
    "    filename=\"figures/abs_err_scp_s.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2641073d5019efd",
   "metadata": {},
   "source": [
    "Visualize the empirical cumulative distribution function of the scores and the threshold given by $\\hat{q}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26de7347822a4a7d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T16:49:39.543143Z",
     "start_time": "2025-08-05T16:49:39.540563Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_cdf(data, cdf, ax, color=\"tab:green\"):\n",
    "    ax.hlines(y=cdf[:-1], xmin=data[:-1], xmax=data[1:], color=color, zorder=1)\n",
    "    ax.vlines(\n",
    "        x=data,\n",
    "        ymin=np.insert(cdf, 0, 0)[:-1],\n",
    "        ymax=cdf,\n",
    "        color=color,\n",
    "        linestyle='dotted',\n",
    "        zorder=1\n",
    "    )\n",
    "    ax.scatter(data, cdf, color=color, s=18, zorder=2)\n",
    "    ax.scatter(data, np.insert(cdf.numpy(), 0, 0)[:-1], color='white', s=18, zorder=2, edgecolor=color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6912487c97c6b8da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T16:49:39.559076Z",
     "start_time": "2025-08-05T16:49:39.555971Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_scp_cdf(scores, emp_q, alpha, filename=None, old_style=False):\n",
    "    fig, ax = plt.subplots()\n",
    "    scores, _ = torch.sort(scores)\n",
    "    cdf = torch.arange(1, len(scores) + 1) / len(scores)\n",
    "    \n",
    "    if old_style:\n",
    "        plt.hist(scores, bins=len(scores), \n",
    "                 alpha=0.7, color=\"tab:green\", edgecolor=\"tab:green\", density=True, cumulative=True)\n",
    "        plt.margins(x=0)\n",
    "    else:\n",
    "        plot_cdf(scores, cdf, ax)\n",
    "    \n",
    "    ax.axvline(\n",
    "        emp_q,\n",
    "        color='red',\n",
    "        linestyle='dashed',\n",
    "        linewidth=1,\n",
    "        label=r'$\\hat{q}$ for $\\alpha=$' + f'{alpha}'  \n",
    "    )\n",
    "    n = len(scores)\n",
    "    coverage_level = (1 - alpha) * (n + 1) / n\n",
    "    ax.axhline(\n",
    "        coverage_level,\n",
    "        color='tab:orange',\n",
    "        linestyle='dashed',\n",
    "        linewidth=1,\n",
    "        label=r'$(1-\\alpha)(n+1)/n$'\n",
    "    )\n",
    "    ax.set_xlabel(r\"scores $\\{s_i\\}_{i=1}^n$\")\n",
    "    ax.set_ylabel(\"cumulative probability\")\n",
    "    ax.grid(True)\n",
    "    ax.legend(loc=4)\n",
    "    if(filename):\n",
    "        fig.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8866031d9de7f505",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T16:49:40.012298Z",
     "start_time": "2025-08-05T16:49:39.573602Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plot_scp_cdf(abs_err_scp.cal_scores, abs_err_scp.emp_q, alpha_example)\n",
    "plot_scp_cdf(abs_err_scp.cal_scores, abs_err_scp.emp_q, alpha_example, filename=\"figures/abs_err_scp_cdf.png\", old_style=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fd7d2ed81e348c",
   "metadata": {},
   "source": [
    "Visualize the predictions and the prediction sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff5b5ecd0a8ec15",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T16:49:40.031301Z",
     "start_time": "2025-08-05T16:49:40.028378Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_scp_predictions(x_test, y_test, y_model_test, prediction_sets, alpha, filename=None):\n",
    "    plt.plot(x_test, y_model_test, label=\"model\", color=\"brown\")\n",
    "    plt.plot(x_test, y_test, '.', label=\"test data\", alpha=0.5)\n",
    "    plt.fill_between(\n",
    "        x_test, prediction_sets[0], prediction_sets[1], alpha=0.2, label=r\"$\\alpha=\\;$%.2f prediction interval\" % alpha\n",
    "    )\n",
    "    plt.grid()\n",
    "    plt.xlabel(\"$x$\")\n",
    "    plt.ylabel(\"$y$\")\n",
    "    plt.legend()\n",
    "    plt.margins(x=0)\n",
    "    if filename:\n",
    "        plt.savefig(filename, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb82a1727c29ce60",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T16:49:40.316734Z",
     "start_time": "2025-08-05T16:49:40.049136Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_scp_predictions(\n",
    "    x_test_example,\n",
    "    y_test_example,\n",
    "    y_model_test_example,\n",
    "    abs_err_scp_prediction_sets,\n",
    "    alpha_example,\n",
    "    filename=\"figures/abs_err_scp_predictions.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75639c033a03dce",
   "metadata": {},
   "source": [
    "For evaluation, we check the coverage of the prediction sets and the average width of the prediction sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb62b93be72ba863",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T16:49:40.324249Z",
     "start_time": "2025-08-05T16:49:40.321541Z"
    }
   },
   "outputs": [],
   "source": [
    "def eval_scp(prediction_sets, y_test):\n",
    "    lower, upper = prediction_sets\n",
    "    coverage = (torch.greater_equal(y_test, lower) & torch.greater_equal(upper, y_test)).float().mean().item()\n",
    "    width = torch.mean(upper - lower).item()\n",
    "    return coverage, width\n",
    "\n",
    "def print_eval_stats(eval_stats):\n",
    "    for method_name, stats in eval_stats.items():\n",
    "        coverage, width = stats\n",
    "        print(method_name)\n",
    "        print(f\"Coverage: {coverage:.2f}, Average width: {width:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3c5cdeb90f030a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T16:49:40.356133Z",
     "start_time": "2025-08-05T16:49:40.350343Z"
    }
   },
   "outputs": [],
   "source": [
    "eval_stats_example = {\"Absolute Error SCP\": eval_scp(abs_err_scp_prediction_sets, y_test_example)}\n",
    "print_eval_stats(eval_stats_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26acacd89badea4c",
   "metadata": {},
   "source": [
    "### Signed-Error SCP\n",
    "\n",
    "The absolute error SCP is not adaptive to asymmetric error distributions, i.e., it does not distinguish between over- and under-predictions. To address this, we can use the signed error as the score function, which allows us to adapt the prediction sets to the distribution of errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a0b5f0fe3aef11",
   "metadata": {},
   "source": [
    "To make the method adaptable to asymmetric error distributions, we can replace the score with asymmetric conformal bands. Specifically, instead of defining $s(x,y)=|y-\\hat{f}(x)|$, two separate scores are used\n",
    "$$\n",
    "    s_{i,\\text{lo}} = s_{\\text{lo}}(x_i,y_i) =  \\hat{f}(x_i) -  y_i, \\quad s_{i,\\text{hi}} = s_{\\text{hi}}(x_i,y_i) = y_i - \\hat{f}(x_i),\n",
    "$$\n",
    "for which we run the conformal prediction at two miscoverage levels $\\alpha_{\\text{lo}},\\alpha_{\\text{hi}}$, resulting in $\\hat{q}_{\\alpha_{\\text{lo}}}$ and $\\hat{q}_{\\alpha_{\\text{hi}}}$, and since both are valid score function we have that\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "    \\mathbb{P}(Y_{\\text{test}} \\in \\mathcal{C}_{\\text{lo}} (X_{\\text{test}})) &= \\mathbb{P}(Y_{\\text{test}} \\geq \\hat{f}(X_{\\text{test}}) - \\hat{q}_{\\alpha_{\\text{lo}}}) \\geq 1-\\alpha_{\\text{lo}},\\\\\n",
    "    \\mathbb{P}Y_{\\text{test}} \\in \\mathcal{C}_{\\text{hi}} (X_{\\text{test}})) &= \\mathbb{P}(Y_{\\text{test}} \\leq \\hat{f}(X_{\\text{test}}) + \\hat{q}_{\\alpha_{\\text{hi}}}) \\geq 1-\\alpha_{\\text{hi}}.\n",
    "\\end{split}\n",
    "\\end{equation*}\n",
    "Importantly, if we define the final confidence set as\n",
    "$$\n",
    "    \\mathcal{C}(X_{\\text{test}}) = [\\hat{f}(X_{\\text{test}})-\\hat{q}_{\\alpha_{\\text{lo}}}, \\hat{f}(X_{\\text{test}}) + \\hat{q}_{\\alpha_{\\text{hi}}}]\n",
    "$$\n",
    "and $\\alpha = \\alpha_{\\text{lo}} + \\alpha_{\\text{hi}}$, by the union bound on miscoverage probability we get\n",
    "$$\n",
    "    \\mathbb{P}(Y_{\\text{test}}\\in\\mathcal{C}(X_{\\text{test}})) \\geq 1-\\alpha.\n",
    "$$\n",
    "\n",
    "The method was introduced by H. Linusson, U. Johansson, and T. Löfström (2014), \"Signed-error conformal regression\", *Advances in Knowledge Discovery and Data Mining (PKDD)*, 224–236."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fe3f2e157a9d41",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T16:49:40.378988Z",
     "start_time": "2025-08-05T16:49:40.373982Z"
    }
   },
   "outputs": [],
   "source": [
    "class SignedErrorSCP:\n",
    "    class OneSidedSCP(BaseSCP):\n",
    "        def __init__(self, alpha):\n",
    "            super().__init__(alpha)\n",
    "\n",
    "        def score(self, y, y_pred):\n",
    "            return y_pred - y\n",
    "\n",
    "        def predict(self, x):\n",
    "            pass\n",
    "\n",
    "    def __init__(self, model, alpha_lo, alpha_hi):\n",
    "        self.model = model\n",
    "        self.alpha_lo = alpha_lo\n",
    "        self.alpha_hi = alpha_hi\n",
    "        self.x_cal = None\n",
    "        self.y_cal = None\n",
    "        self.scp_lo = None\n",
    "        self.scp_hi = None\n",
    "\n",
    "    def fit(self, x_cal, y_cal):\n",
    "        self.x_cal = x_cal\n",
    "        self.y_cal = y_cal\n",
    "        with torch.no_grad():\n",
    "            y_cal_pred = self.model(x_cal)\n",
    "\n",
    "        # Fit lower bound SCP\n",
    "        self.scp_lo = self.OneSidedSCP(self.alpha_lo)\n",
    "        self.scp_lo.fit(y_cal, y_cal_pred)\n",
    "\n",
    "        # Fit upper bound SCP\n",
    "        self.scp_hi = self.OneSidedSCP(self.alpha_hi)\n",
    "        self.scp_hi.fit(-y_cal, -y_cal_pred)\n",
    "\n",
    "    def predict(self, x_test):\n",
    "        with torch.no_grad():\n",
    "            y_test_pred = self.model(x_test)\n",
    "        return y_test_pred - self.scp_lo.emp_q, y_test_pred + self.scp_hi.emp_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c503ed44bec025",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T16:49:40.397257Z",
     "start_time": "2025-08-05T16:49:40.393354Z"
    }
   },
   "outputs": [],
   "source": [
    "signed_err_scp = SignedErrorSCP(model_example, alpha_example / 2, alpha_example / 2)\n",
    "signed_err_scp.fit(x_cal_example, y_cal_example)\n",
    "signed_err_scp_prediction_sets = signed_err_scp.predict(x_test_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4911047a993ec67",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T16:49:40.573352Z",
     "start_time": "2025-08-05T16:49:40.412211Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_scp_predictions(\n",
    "    x_test_example,\n",
    "    y_test_example,\n",
    "    y_model_test_example,\n",
    "    signed_err_scp_prediction_sets,\n",
    "    alpha_example,\n",
    "    filename=\"figures/signed_err_scp_predictions.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242096a900b1166c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T16:49:40.583693Z",
     "start_time": "2025-08-05T16:49:40.579919Z"
    }
   },
   "outputs": [],
   "source": [
    "eval_stats_example[\"Signed Error SCP\"] = eval_scp(signed_err_scp_prediction_sets, y_test_example)\n",
    "print_eval_stats(eval_stats_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996517d4-7bb7-4b9d-9d6b-cd90031716fd",
   "metadata": {},
   "source": [
    "### Scaled residual SCP\n",
    "So far, the uncertainty estimate was independent of $x$.\n",
    "\n",
    "Let's make the score $s(x, y)$ depend on $u(x)$, say\n",
    "$$\n",
    "s(x,y)=\\frac{|y-f(x)|}{u(x)}\\enspace.\n",
    "$$\n",
    "Then when going from $\\hat{q}$ to the prediction interval for some $x$\n",
    "then the $\\hat{q}$ is scaled by $u(x)$.\n",
    "\n",
    "This is an important setting for conformal prediction, because it allows to calibrate uncertainty estimates ($u(x)$) coming from another method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3271277b-a2e7-45ee-bde3-414e4b0162f0",
   "metadata": {},
   "source": [
    "Let's assume we know that we have an estimator $\\hat{l}(x)$ of the logarithm of the heteroscedastic noise variance.\n",
    "We can learn an estimator of the noise variance using the <code>GaussianNLLLoss</code> loss function. To ensure positivity of the variance, we encode it by its logarithm.\n",
    "\n",
    "Given an estimate of the variance, we can use the estimated standard deviation as uncertainty estimate $u(x)=\\sqrt{\\exp{\\hat{l}}}$ to define\n",
    "$s(x,y)=\\frac{|y-f(x)|}{u(x)}= \\frac{|y-f(x)|}{\\sqrt{\\exp{\\hat{l}}}}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5831b7cd65e79ec9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T16:49:40.602411Z",
     "start_time": "2025-08-05T16:49:40.599067Z"
    }
   },
   "outputs": [],
   "source": [
    "class ScaledResidualLogVarSCP(BaseSCP):\n",
    "    def __init__(self, model, model_logvar, alpha):\n",
    "        self.model = model\n",
    "        self.model_logvar = model_logvar\n",
    "        super().__init__(alpha)\n",
    "\n",
    "    def score(self, x, y):\n",
    "        with torch.no_grad():\n",
    "            y_hat = self.model(x)\n",
    "            log_var = self.model_logvar(x)\n",
    "            u = torch.sqrt(torch.exp(log_var))\n",
    "            return torch.abs(y - y_hat) / u \n",
    "\n",
    "    def predict(self, x_test):\n",
    "        with torch.no_grad():\n",
    "            y_test_pred = self.model(x_test)\n",
    "            y_test_logvar = self.model_logvar(x_test)\n",
    "            u_test = torch.sqrt(torch.exp(y_test_logvar))\n",
    "        return y_test_pred - self.emp_q * u_test, y_test_pred + self.emp_q * u_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff18be0-0d9f-49e9-9c14-a5eb14576657",
   "metadata": {},
   "source": [
    "To get estimator $\\hat{l}(x)$ of the logarithm of the heteroscedastic noise variance, \n",
    "we train an MLP using the <code>GaussianNLLLoss</code> loss function, see\n",
    "D. A. Nix and A. S. Weigend (1994), \"Estimating the mean and variance of the target\n",
    "probability distribution\", *International Conference on Neural Networks (ICNN)*,\n",
    "vol. 1, IEEE, 55–60."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8516ef44-b258-42c2-8126-7f6ed1fcc216",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, input_dim=1, n_hidden=16):\n",
    "        super(MLP, self).__init__()\n",
    "        self.hidden = nn.Linear(input_dim, n_hidden)\n",
    "        self.log_var = nn.Linear(n_hidden, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim()==1:\n",
    "            x=x.view(-1,1)\n",
    "        x = nn.functional.elu(self.hidden(x))\n",
    "        log_var = self.log_var(x)\n",
    "        return torch.squeeze(log_var)\n",
    "\n",
    "def fit_log_var_MLP(x, y, model, lr=0.01, num_epochs = 2000, verbose=False):\n",
    "    model_log_var = MLP()\n",
    "\n",
    "    criterion = nn.GaussianNLLLoss()\n",
    "    optimizer = torch.optim.Rprop(model_log_var.parameters(), lr=lr, etas=(0.5, 1.1), step_sizes=(0, 5))\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(x)\n",
    "        log_var = model_log_var(x)\n",
    "        var = torch.exp(log_var)  # Convert log-variance to variance\n",
    "        loss = criterion(y_pred, y, var)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    return model_log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23a3ecd-7d11-44dd-9867-e71d436ec5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float64)\n",
    "model_log_var = fit_log_var_MLP(x_train_example, y_train_example, model_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826846cbcd3972d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T16:49:40.617907Z",
     "start_time": "2025-08-05T16:49:40.614662Z"
    }
   },
   "outputs": [],
   "source": [
    "scaled_res_scp = ScaledResidualLogVarSCP(model_example, model_log_var, alpha_example)\n",
    "scaled_res_scp.fit(x_cal_example, y_cal_example)\n",
    "scaled_res_scp_prediction_sets = scaled_res_scp.predict(x_test_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d45cdd9091f56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T16:49:40.818236Z",
     "start_time": "2025-08-05T16:49:40.644335Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_scp_predictions(\n",
    "    x_test_example,\n",
    "    y_test_example,\n",
    "    y_model_test_example,\n",
    "    scaled_res_scp_prediction_sets,\n",
    "    alpha_example,\n",
    "    filename=\"figures/scaled_res_scp_predictions.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1447618219770dc0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T16:49:40.825892Z",
     "start_time": "2025-08-05T16:49:40.822940Z"
    }
   },
   "outputs": [],
   "source": [
    "eval_stats_example[\"Scaled Residual SCP\"] = eval_scp(scaled_res_scp_prediction_sets, y_test_example)\n",
    "print_eval_stats(eval_stats_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90eb4035b4de7f5",
   "metadata": {},
   "source": [
    "### Conformalized Quantile Regression (CQR)\n",
    "\n",
    "Conformalized Quantile Regression (CQR) is a method that uses quantile regression to estimate the lower and upper bounds of the prediction sets. It fits two quantile regression models, one for the lower quantile and one for the upper quantile, and uses these models to form the prediction sets.\n",
    "\n",
    "The method was proposed by Y. Romano, E. Patterson, and E. Candes (2019), \"Conformalized quantile regression\", *Advances in Neural Information Processing Systems (NeurIPS)*, vol. 32.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3308308ae65effc",
   "metadata": {},
   "source": [
    "We fit the quantile regression models using the pinball loss\n",
    "$$\n",
    "\\ell_{\\text{pinball}}(y, \\hat{f}(x)) = \\max(\\tau(y - \\hat{f}(x)), (1-\\tau)(\\hat{f}(x) - y)),\n",
    "$$\n",
    "where $\\tau$ is the quantile level. \n",
    "\n",
    "We use simple MLPs to learn the $\\tau$-quantiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75efa08c3385fc2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T16:49:40.854035Z",
     "start_time": "2025-08-05T16:49:40.849918Z"
    }
   },
   "outputs": [],
   "source": [
    "class PinballLoss(nn.Module):\n",
    "    def __init__(self, quantile: float):\n",
    "        super().__init__()\n",
    "        self.quantile = quantile\n",
    "\n",
    "    def forward(self, y_pred: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:\n",
    "        errors = y_true - y_pred\n",
    "        loss = torch.max(self.quantile * errors, (self.quantile - 1) * errors)\n",
    "        return torch.mean(loss)\n",
    "\n",
    "def fit_power_quantile(x, y, q, a=1., b=1., lr=0.01, num_epochs = 2000, verbose=False):\n",
    "    model = MLP() \n",
    "\n",
    "    criterion = PinballLoss(q)\n",
    "    optimizer = torch.optim.Rprop(model.parameters(), lr=lr, etas=(0.5, 1.1))\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        y_pred = model(x)\n",
    "        loss = criterion(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        if verbose:\n",
    "            print('epoch {}, RMSE {}'.format(epoch, torch.sqrt(loss).item()))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c480a962ece83006",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T16:49:41.386446Z",
     "start_time": "2025-08-05T16:49:40.867542Z"
    }
   },
   "outputs": [],
   "source": [
    "lower_q_model_example = fit_power_quantile(x_train_example, y_train_example, 0.05, verbose=True)\n",
    "upper_q_model_example = fit_power_quantile(x_train_example, y_train_example, 0.95, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1cebd0dc61eaf9",
   "metadata": {},
   "source": [
    "Define the CQR class that uses the two quantile regression models to form the prediction sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe46cff7c9b7a346",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T16:49:41.400175Z",
     "start_time": "2025-08-05T16:49:41.396637Z"
    }
   },
   "outputs": [],
   "source": [
    "class CQR(BaseSCP):\n",
    "    def __init__(self, lower_q_model, upper_q_model, alpha):\n",
    "        super().__init__(alpha)\n",
    "        self.lower_q_model = lower_q_model\n",
    "        self.upper_q_model = upper_q_model\n",
    "\n",
    "    def score(self, x, y):\n",
    "        with torch.no_grad():\n",
    "            lower_q_pred = self.lower_q_model(x)\n",
    "            upper_q_pred = self.upper_q_model(x)\n",
    "        return torch.maximum(lower_q_pred - y, y - upper_q_pred)\n",
    "\n",
    "    def predict(self, x):\n",
    "        with torch.no_grad():\n",
    "            lower_q_pred = self.lower_q_model(x)\n",
    "            upper_q_pred = self.upper_q_model(x)\n",
    "        return lower_q_pred - self.emp_q, upper_q_pred + self.emp_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96771170fe2f2500",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T16:49:41.420269Z",
     "start_time": "2025-08-05T16:49:41.413865Z"
    }
   },
   "outputs": [],
   "source": [
    "cqr = CQR(lower_q_model_example, upper_q_model_example, alpha_example)\n",
    "cqr.fit(x_cal_example, y_cal_example)\n",
    "cqr_prediction_sets = cqr.predict(x_test_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8519c8e2245b7efb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T16:49:41.745842Z",
     "start_time": "2025-08-05T16:49:41.453480Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_scp_predictions(\n",
    "    x_test_example,\n",
    "    y_test_example,\n",
    "    y_model_test_example,\n",
    "    cqr_prediction_sets,\n",
    "    alpha_example,\n",
    "    filename=\"figures/cqr_predictions.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fecfbc67411fead",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T16:49:41.771662Z",
     "start_time": "2025-08-05T16:49:41.767456Z"
    }
   },
   "outputs": [],
   "source": [
    "eval_stats_example[\"Conformalized Quantile Regression\"] = eval_scp(cqr_prediction_sets, y_test_example)\n",
    "print_eval_stats(eval_stats_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bda065-40b7-48a9-b201-1ee3ff3125cf",
   "metadata": {},
   "source": [
    "### Split Localized Conformal Prediction (SLCP)\n",
    "Split Localized Conformal Prediction (SLCP) is a method that combines the ideas of localization and split conformal prediction. It uses a kernel-based approach to compute weights for the training data, which allows it to adapt the quantile estimates to the local distribution of the data.\n",
    "\n",
    "The method was proposed by X. Han, Z. Tang, J. Ghosh, and Q. Liu (2023), \"Split Localized Conformal Prediction\", arXiv: 2206.13092."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d2e03105f6d5db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T16:49:41.817570Z",
     "start_time": "2025-08-05T16:49:41.810671Z"
    }
   },
   "outputs": [],
   "source": [
    "class SplitLocalizedCP:\n",
    "    class LocalizedCP(BaseSCP):\n",
    "        def __init__(self, alpha):\n",
    "            super().__init__(alpha)\n",
    "\n",
    "        def score(self, scores, localized_q):\n",
    "            return scores - localized_q\n",
    "\n",
    "        def predict(self, x):\n",
    "            pass\n",
    "\n",
    "    def __init__(self, model, alpha_lo, alpha_hi, kernel, kernel_bandwidth):\n",
    "        self.model = model\n",
    "        self.alpha_lo = alpha_lo\n",
    "        self.alpha_hi = alpha_hi\n",
    "        self.kernel = kernel\n",
    "        self.kernel_bandwidth = kernel_bandwidth\n",
    "        self.x_train = None\n",
    "        self.y_train = None\n",
    "        self.x_cal = None\n",
    "        self.y_cal = None\n",
    "        self.cp_q_lo = None\n",
    "        self.cp_q_hi = None\n",
    "        self.train_scores_lo = None\n",
    "\n",
    "    def __compute_weights(self, x):\n",
    "        if len(x.shape) == 1:\n",
    "            x = x[:, None]  # Ensure x is 2D for consistency\n",
    "        # print(x.shape)\n",
    "        n_train = len(self.x_train)\n",
    "        x_rep = x[:, None].repeat(1, n_train, 1)\n",
    "        n = len(x)\n",
    "        x_train_rep = self.x_train[None, :].repeat(n, 1, 1)\n",
    "        # print(x_rep.shape, x_train_rep.shape)\n",
    "        norms = torch.linalg.norm(x_rep - x_train_rep, dim=-1) / self.kernel_bandwidth\n",
    "        k = self.kernel(norms.flatten()).reshape(n, n_train)\n",
    "        # print(k)\n",
    "        return k / k.sum(dim=1, keepdims=True)\n",
    "\n",
    "    def __get_localized_q(self, weights, alpha, train_scores):\n",
    "        train_s_arg_sorted = torch.argsort(train_scores)\n",
    "        emp_cdf = torch.cumsum(weights[:, train_s_arg_sorted], dim=1)\n",
    "        # print(weights)\n",
    "        localized_q_indices = torch.argmax(((1-alpha) <= emp_cdf).float(), dim=1)\n",
    "        localized_qs = train_scores[train_s_arg_sorted[localized_q_indices]]\n",
    "        return localized_qs\n",
    "\n",
    "    def __get_conformal_q(self, alpha, cal_scores, localized_qs):\n",
    "        localized_cp = self.LocalizedCP(alpha)\n",
    "        localized_cp.fit(cal_scores, localized_qs)\n",
    "        return localized_cp.emp_q\n",
    "\n",
    "    def fit(self, x_train, y_train, x_cal, y_cal):\n",
    "        with torch.no_grad():\n",
    "            self.train_scores_lo = self.model(x_train) - y_train\n",
    "        if len(x_train.shape) == 1:\n",
    "            x_train = x_train[:, None]  # Ensure x_train is 2D for consistency\n",
    "        self.x_train = x_train\n",
    "        weights = self.__compute_weights(x_cal)\n",
    "        localized_q_lo = self.__get_localized_q(weights, self.alpha_lo, self.train_scores_lo)\n",
    "        localized_q_hi = self.__get_localized_q(weights, self.alpha_hi, -self.train_scores_lo)\n",
    "        # print(localized_q_lo, localized_q_hi)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            cal_scores_lo = self.model(x_cal) - y_cal\n",
    "        self.cp_q_lo = self.__get_conformal_q(self.alpha_lo, cal_scores_lo, localized_q_lo)\n",
    "        self.cp_q_hi = self.__get_conformal_q(self.alpha_hi, -cal_scores_lo, localized_q_hi)\n",
    "        # print(f\"Localized CP quantiles: {self.cp_q_lo:.4f} (lo), {self.cp_q_hi:.4f} (hi)\")\n",
    "\n",
    "    def predict(self, x):\n",
    "        with torch.no_grad():\n",
    "            y_pred = self.model(x)\n",
    "        weights = self.__compute_weights(x)\n",
    "        localized_q_lo = self.__get_localized_q(weights, self.alpha_lo, self.train_scores_lo)\n",
    "        localized_q_hi = self.__get_localized_q(weights, self.alpha_hi, -self.train_scores_lo)\n",
    "        return y_pred - localized_q_lo - self.cp_q_lo, y_pred + localized_q_hi + self.cp_q_hi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b0d597185b1b5d",
   "metadata": {},
   "source": [
    "Define the kernel function and a heuristic for choosing the bandwidth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df131d9f719df6ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T16:49:41.841562Z",
     "start_time": "2025-08-05T16:49:41.837044Z"
    }
   },
   "outputs": [],
   "source": [
    "def gaussian_kernel(x):\n",
    "    return 1 / torch.sqrt(torch.tensor(2 * torch.pi)) * torch.exp(-0.5 * x ** 2)\n",
    "\n",
    "def compute_median_trick_bandwidth(features) -> float:\n",
    "    # pdist computes the pairwise distances between all rows and returns a\n",
    "    # condensed distance matrix (a 1D array).\n",
    "    pairwise_distances = pdist(features, 'euclidean')\n",
    "\n",
    "    # return the median of these distances.\n",
    "    return np.median(pairwise_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57befa861793ccdf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T16:49:41.872053Z",
     "start_time": "2025-08-05T16:49:41.864154Z"
    }
   },
   "outputs": [],
   "source": [
    "kernel_bandwidth_example = compute_median_trick_bandwidth(x_train_example[:, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3be97a196fce25f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T16:49:41.896413Z",
     "start_time": "2025-08-05T16:49:41.890644Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"kernel bandwidth:\", kernel_bandwidth_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be188fa62fd13da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T16:49:41.934067Z",
     "start_time": "2025-08-05T16:49:41.923546Z"
    }
   },
   "outputs": [],
   "source": [
    "slcp = SplitLocalizedCP(\n",
    "    model=model_example,\n",
    "    alpha_lo=alpha_example / 2,\n",
    "    alpha_hi=alpha_example / 2,\n",
    "    kernel=gaussian_kernel,\n",
    "    kernel_bandwidth=kernel_bandwidth_example\n",
    ")\n",
    "slcp.fit(x_train_example, y_train_example, x_cal_example, y_cal_example)\n",
    "slcp_prediction_sets = slcp.predict(x_test_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43f65d9cc9281a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T16:49:42.131672Z",
     "start_time": "2025-08-05T16:49:41.953030Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_scp_predictions(\n",
    "    x_test_example,\n",
    "    y_test_example,\n",
    "    y_model_test_example,\n",
    "    slcp_prediction_sets,\n",
    "    alpha_example,\n",
    "    filename=\"figures/slcp_predictions.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9ab3bfd7dd611d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T16:49:42.157233Z",
     "start_time": "2025-08-05T16:49:42.154165Z"
    }
   },
   "outputs": [],
   "source": [
    "eval_stats_example[\"Split Localized Conformal Prediction\"] = eval_scp(slcp_prediction_sets, y_test_example)\n",
    "print_eval_stats(eval_stats_example)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
